{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename)) \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport  torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-20T15:50:42.902911Z","iopub.execute_input":"2024-07-20T15:50:42.904075Z","iopub.status.idle":"2024-07-20T15:50:45.775637Z","shell.execute_reply.started":"2024-07-20T15:50:42.904038Z","shell.execute_reply":"2024-07-20T15:50:45.774496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tensors\nAs the name suggests,PyTorch is a library for processing Tensors.A tensor is a multidimensional array,number,vector or a matrix.\n\nJust like NumPy arrays, tensors have a\ntype and a shape. In fact, in the Python API tensors are simply represented by NumPy\nndarrays. They typically contain floats, but you can also use them to carry strings\n(arbitrary byte arrays).\n\nTensors can have any number of dimensions aand different lengths along each dimension.We can inspect the length along each dimension using the .shape property of tensor.\n\nNOTE that it's not possible to create tensors with an improper shape","metadata":{}},{"cell_type":"code","source":"t1=torch.tensor(4.)\nt1","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.777741Z","iopub.execute_input":"2024-07-20T15:50:45.778640Z","iopub.status.idle":"2024-07-20T15:50:45.876714Z","shell.execute_reply.started":"2024-07-20T15:50:45.778577Z","shell.execute_reply":"2024-07-20T15:50:45.875687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. is a shorthand for 4.0. used to indicate pytorch that you want to create a floating point number","metadata":{}},{"cell_type":"code","source":"#vector\nt2=torch.tensor([1.,2,3,4])\nt2","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.878043Z","iopub.execute_input":"2024-07-20T15:50:45.878373Z","iopub.status.idle":"2024-07-20T15:50:45.886154Z","shell.execute_reply.started":"2024-07-20T15:50:45.878346Z","shell.execute_reply":"2024-07-20T15:50:45.885121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"all the elemnts of a tensor have same type","metadata":{}},{"cell_type":"code","source":"#matrix\n\nt3=torch.tensor([[5.,6],[3,4],[88,9]])\nt3","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.889128Z","iopub.execute_input":"2024-07-20T15:50:45.889567Z","iopub.status.idle":"2024-07-20T15:50:45.898934Z","shell.execute_reply.started":"2024-07-20T15:50:45.889529Z","shell.execute_reply":"2024-07-20T15:50:45.897682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3-d array....gonna give you a cuboid kinda structure\nt4=torch.tensor([[[34,55,66],[33,45,6]],[[33,9,8],[22,11,82]]])\nt4","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.900448Z","iopub.execute_input":"2024-07-20T15:50:45.901167Z","iopub.status.idle":"2024-07-20T15:50:45.910131Z","shell.execute_reply.started":"2024-07-20T15:50:45.901119Z","shell.execute_reply":"2024-07-20T15:50:45.908973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tensor operations and gradients","metadata":{}},{"cell_type":"code","source":"#creating tensors\nx=torch.tensor(3.)\nw=torch.tensor(4.,requires_grad=True)\nb=torch.tensor(5.,requires_grad=True)\nx,w,b","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.911684Z","iopub.execute_input":"2024-07-20T15:50:45.912026Z","iopub.status.idle":"2024-07-20T15:50:45.922274Z","shell.execute_reply.started":"2024-07-20T15:50:45.911999Z","shell.execute_reply":"2024-07-20T15:50:45.921125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#arithmetic operation\ny=w*x+b\ny","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.923698Z","iopub.execute_input":"2024-07-20T15:50:45.924042Z","iopub.status.idle":"2024-07-20T15:50:45.939351Z","shell.execute_reply.started":"2024-07-20T15:50:45.924013Z","shell.execute_reply":"2024-07-20T15:50:45.938161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pytorch is unique as we can automatically compute the derivaitve of y w.r.t the tensor that have requires_grad set to true. This feature is called automatic gradient","metadata":{}},{"cell_type":"code","source":"#computing derivatives\ny.backward()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.940791Z","iopub.execute_input":"2024-07-20T15:50:45.941162Z","iopub.status.idle":"2024-07-20T15:50:45.957652Z","shell.execute_reply.started":"2024-07-20T15:50:45.941131Z","shell.execute_reply":"2024-07-20T15:50:45.956703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display gradients\nprint('dy/dx:',x.grad)\nprint('dy/dw:',w.grad)\nprint('dy/db:',b.grad)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.958878Z","iopub.execute_input":"2024-07-20T15:50:45.959244Z","iopub.status.idle":"2024-07-20T15:50:45.966232Z","shell.execute_reply.started":"2024-07-20T15:50:45.959215Z","shell.execute_reply":"2024-07-20T15:50:45.965248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tensor functions","metadata":{}},{"cell_type":"code","source":"#create a tensor with a fixed value for every element\nt6=torch.full((3,2),33)\nt6","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.970226Z","iopub.execute_input":"2024-07-20T15:50:45.970591Z","iopub.status.idle":"2024-07-20T15:50:45.982278Z","shell.execute_reply.started":"2024-07-20T15:50:45.970561Z","shell.execute_reply":"2024-07-20T15:50:45.981261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concatenate two tensors with compatible shapes\nt7=torch.cat((t3,t6))\nt7","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.983959Z","iopub.execute_input":"2024-07-20T15:50:45.984291Z","iopub.status.idle":"2024-07-20T15:50:45.996833Z","shell.execute_reply.started":"2024-07-20T15:50:45.984263Z","shell.execute_reply":"2024-07-20T15:50:45.995764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compute the sin of each element\nt8=torch.sin(t7)\nt8","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:45.998832Z","iopub.execute_input":"2024-07-20T15:50:45.999724Z","iopub.status.idle":"2024-07-20T15:50:46.014700Z","shell.execute_reply.started":"2024-07-20T15:50:45.999684Z","shell.execute_reply":"2024-07-20T15:50:46.013674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change the tensor shape\nt9=t8.reshape(3,2,2)\nt9","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:46.015933Z","iopub.execute_input":"2024-07-20T15:50:46.016257Z","iopub.status.idle":"2024-07-20T15:50:46.024265Z","shell.execute_reply.started":"2024-07-20T15:50:46.016230Z","shell.execute_reply":"2024-07-20T15:50:46.023015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=np.array([[1,2],[2,5],[5.,6]])\nx","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:46.025726Z","iopub.execute_input":"2024-07-20T15:50:46.026090Z","iopub.status.idle":"2024-07-20T15:50:46.034900Z","shell.execute_reply.started":"2024-07-20T15:50:46.026061Z","shell.execute_reply":"2024-07-20T15:50:46.033676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert the numpy array to a PyTorch tensor \ny=torch.from_numpy(x)\ny","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:46.036310Z","iopub.execute_input":"2024-07-20T15:50:46.037044Z","iopub.status.idle":"2024-07-20T15:50:46.048849Z","shell.execute_reply.started":"2024-07-20T15:50:46.037003Z","shell.execute_reply":"2024-07-20T15:50:46.047697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert a torch to a numpy array \nz=y.numpy()\nz\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:46.050024Z","iopub.execute_input":"2024-07-20T15:50:46.050391Z","iopub.status.idle":"2024-07-20T15:50:46.059113Z","shell.execute_reply.started":"2024-07-20T15:50:46.050357Z","shell.execute_reply":"2024-07-20T15:50:46.058026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reasons why we need PyTorch since Numpy alread provides data structures and utillites for working with mutli dimensional numeric data\n1. Autograd-the ability to automatically compute gradients for tensor operations is essential for training deep learning models\n1. GPU support-while working with massive datasets amd large models,pytorch tensor operations can be performed efficiently using a Graphic Processing Unit.computations that might take hours can be completed within minutes using GPUs","metadata":{}},{"cell_type":"markdown","source":"# WORKING WITH MNIST HANDWRITTEN DATABASE","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision.datasets import MNIST","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:46.060318Z","iopub.execute_input":"2024-07-20T15:50:46.060667Z","iopub.status.idle":"2024-07-20T15:50:47.858787Z","shell.execute_reply.started":"2024-07-20T15:50:46.060636Z","shell.execute_reply":"2024-07-20T15:50:47.857593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#downloading training dataset\ndataset=MNIST(root='data/',download=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:47.860048Z","iopub.execute_input":"2024-07-20T15:50:47.860369Z","iopub.status.idle":"2024-07-20T15:50:50.319502Z","shell.execute_reply.started":"2024-07-20T15:50:47.860342Z","shell.execute_reply":"2024-07-20T15:50:50.318429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.321092Z","iopub.execute_input":"2024-07-20T15:50:50.321673Z","iopub.status.idle":"2024-07-20T15:50:50.327463Z","shell.execute_reply.started":"2024-07-20T15:50:50.321643Z","shell.execute_reply":"2024-07-20T15:50:50.326419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset=MNIST(root='data/',train=False)\ntest_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.329137Z","iopub.execute_input":"2024-07-20T15:50:50.329540Z","iopub.status.idle":"2024-07-20T15:50:50.349818Z","shell.execute_reply.started":"2024-07-20T15:50:50.329504Z","shell.execute_reply":"2024-07-20T15:50:50.348707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline \n#without this the jupyter will show the graphs as a popups#","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.351035Z","iopub.execute_input":"2024-07-20T15:50:50.351336Z","iopub.status.idle":"2024-07-20T15:50:50.358812Z","shell.execute_reply.started":"2024-07-20T15:50:50.351310Z","shell.execute_reply":"2024-07-20T15:50:50.357678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image,label= dataset[0]\nplt.imshow(image,cmap='gray')\nprint('label:',label)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.360061Z","iopub.execute_input":"2024-07-20T15:50:50.360389Z","iopub.status.idle":"2024-07-20T15:50:50.594781Z","shell.execute_reply.started":"2024-07-20T15:50:50.360363Z","shell.execute_reply":"2024-07-20T15:50:50.593629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"it's evident that it can be challenging to recognize the images with human eye.\nPyTorch doesn't know how to work with images.We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset\nwe can see it is 28x28 pixels image","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms as transforms","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.596109Z","iopub.execute_input":"2024-07-20T15:50:50.596415Z","iopub.status.idle":"2024-07-20T15:50:50.601230Z","shell.execute_reply.started":"2024-07-20T15:50:50.596389Z","shell.execute_reply":"2024-07-20T15:50:50.600148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MNIST dataset (images and labels)\ndataset = MNIST(root='data/', \n                train=True,\n                transform=transforms.ToTensor())","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.602806Z","iopub.execute_input":"2024-07-20T15:50:50.603254Z","iopub.status.idle":"2024-07-20T15:50:50.693316Z","shell.execute_reply.started":"2024-07-20T15:50:50.603217Z","shell.execute_reply":"2024-07-20T15:50:50.692324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_tensor, label = dataset[0]\nprint(img_tensor.shape, label)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.694483Z","iopub.execute_input":"2024-07-20T15:50:50.694795Z","iopub.status.idle":"2024-07-20T15:50:50.702803Z","shell.execute_reply.started":"2024-07-20T15:50:50.694770Z","shell.execute_reply":"2024-07-20T15:50:50.701672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's look at the sample values inside the tensor","metadata":{}},{"cell_type":"code","source":"print(img_tensor[0,10:15,10:15])\nprint(torch.max(img_tensor),torch.min(img_tensor))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.704208Z","iopub.execute_input":"2024-07-20T15:50:50.705227Z","iopub.status.idle":"2024-07-20T15:50:50.714565Z","shell.execute_reply.started":"2024-07-20T15:50:50.705186Z","shell.execute_reply":"2024-07-20T15:50:50.713308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The values range fromm 0 to 1 with 0 representing black,1 white and the values in between are different shades of grey. We can plot the tensor as an image using plt.imshow","metadata":{}},{"cell_type":"code","source":"plt.imshow(img_tensor[0,10:15,10:15],cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.716113Z","iopub.execute_input":"2024-07-20T15:50:50.716562Z","iopub.status.idle":"2024-07-20T15:50:50.945774Z","shell.execute_reply.started":"2024-07-20T15:50:50.716524Z","shell.execute_reply":"2024-07-20T15:50:50.944454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SPLITTING DATA INTO TRAIN VALIDATION AND TEST SET**","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import random_split\ntrain_ds, val_ds=random_split(dataset,[50000,10000])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.953283Z","iopub.execute_input":"2024-07-20T15:50:50.953656Z","iopub.status.idle":"2024-07-20T15:50:50.965406Z","shell.execute_reply.started":"2024-07-20T15:50:50.953602Z","shell.execute_reply":"2024-07-20T15:50:50.964243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"its essential to choose a random sample for creating a validation set. the training data id often sorted by the target labels,i.e images of 0s followed by images of 1s. if we create a val set of the remaninng 20% of images, it would only consists of 8s and 9s. such a split would not make useful models\nwe'll use a batch size of 128","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size=128\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.966847Z","iopub.execute_input":"2024-07-20T15:50:50.967279Z","iopub.status.idle":"2024-07-20T15:50:50.973205Z","shell.execute_reply.started":"2024-07-20T15:50:50.967240Z","shell.execute_reply":"2024-07-20T15:50:50.971974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training model**\nA logistic reegression model is almost identical to a linear regression model. ot contains weights and bias matrices and the output is obtained by simple matrix operations(pred=x@w.t()+b).... @ is matrix mul\n\nwe will use nn.linear to create the model instead of manually creating and initializing matrices \n\neach 1x28x28 image tensor is flattend innto a vector of 784(28*28)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\ninput_size=28*28\nnum_classes=10\nmodel=nn.Linear(input_size,num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.974580Z","iopub.execute_input":"2024-07-20T15:50:50.975718Z","iopub.status.idle":"2024-07-20T15:50:50.987835Z","shell.execute_reply.started":"2024-07-20T15:50:50.975679Z","shell.execute_reply":"2024-07-20T15:50:50.986581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's extend thenn.module class from PyTorch to define a custom model","metadata":{}},{"cell_type":"code","source":"class MnistModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n        \n    def forward(self, xb):\n        xb = xb.reshape(-1, 784)\n        out = self.linear(xb)\n        return out\n    \nmodel = MnistModel()\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.989207Z","iopub.execute_input":"2024-07-20T15:50:50.989528Z","iopub.status.idle":"2024-07-20T15:50:50.996548Z","shell.execute_reply.started":"2024-07-20T15:50:50.989501Z","shell.execute_reply":"2024-07-20T15:50:50.995557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inside the `__init__` constructor method, we instantiate the weights and biases using `nn.Linear`. And inside the `forward` method, which is invoked when we pass a batch of inputs to the model, we flatten the input tensor and pass it into `self.linear`.\n\n`xb.reshape(-1, 28*28)` indicates to PyTorch that we want a *view* of the `xb` tensor with two dimensions. The length along the 2nd dimension is 28\\*28 (i.e., 784). One argument to `.reshape` can be set to `-1` (in this case, the first dimension) to let PyTorch figure it out automatically based on the shape of the original tensor.\n\nNote that the model no longer has `.weight` and `.bias` attributes (as they are now inside the `.linear` attribute), but it does have a `.parameters` method that returns a list containing the weights and bias.","metadata":{}},{"cell_type":"code","source":"for images,labels in train_loader:\n    outputs=model(images)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:50.997703Z","iopub.execute_input":"2024-07-20T15:50:50.998891Z","iopub.status.idle":"2024-07-20T15:50:51.044188Z","shell.execute_reply.started":"2024-07-20T15:50:50.998854Z","shell.execute_reply":"2024-07-20T15:50:51.043054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(outputs.shape)\nprint(outputs[:2].data)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:51.045452Z","iopub.execute_input":"2024-07-20T15:50:51.045774Z","iopub.status.idle":"2024-07-20T15:50:51.052341Z","shell.execute_reply.started":"2024-07-20T15:50:51.045747Z","shell.execute_reply":"2024-07-20T15:50:51.051267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for each of the 100 images input,we get 10 outputs. each row's elements must lie between 0 to 1 and adds up to 1 which is not the case. so we use softmax function","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional  as F","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:51.053767Z","iopub.execute_input":"2024-07-20T15:50:51.054142Z","iopub.status.idle":"2024-07-20T15:50:51.060663Z","shell.execute_reply.started":"2024-07-20T15:50:51.054107Z","shell.execute_reply":"2024-07-20T15:50:51.059695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Apply softmax\nprobs=F.softmax(outputs,dim=1)\nprint('sample prob',probs[:2].data)\nprint(torch.sum(probs[0]).item())","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:51.062064Z","iopub.execute_input":"2024-07-20T15:50:51.062357Z","iopub.status.idle":"2024-07-20T15:50:51.076489Z","shell.execute_reply.started":"2024-07-20T15:50:51.062332Z","shell.execute_reply":"2024-07-20T15:50:51.075415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports\nimport torch\n\nimport torchvision\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import random_split\nfrom torch.utils.data import DataLoader\n# Hyperparmeters\nbatch_size = 128\nlearning_rate = 0.001\n\n# Other constants\ninput_size = 28*28\nnum_classes = 10","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:51.077996Z","iopub.execute_input":"2024-07-20T15:50:51.078334Z","iopub.status.idle":"2024-07-20T15:50:51.085155Z","shell.execute_reply.started":"2024-07-20T15:50:51.078309Z","shell.execute_reply":"2024-07-20T15:50:51.084061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download dataset\ndataset = MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True)\n\n# Training validation & test dataset\ntrain_ds, val_ds = random_split(dataset, [50000, 10000])\ntest_ds = MNIST(root='data/', train=False, transform=transforms.ToTensor())\n\n# Dataloaders\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size*2)\ntest_loader = DataLoader(test_ds, batch_size*2)\nimage, label = train_ds[0]\nplt.imshow(image[0], cmap='gray')\nprint('Label:', label)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:51.086527Z","iopub.execute_input":"2024-07-20T15:50:51.086837Z","iopub.status.idle":"2024-07-20T15:50:51.429225Z","shell.execute_reply.started":"2024-07-20T15:50:51.086813Z","shell.execute_reply":"2024-07-20T15:50:51.428266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MnistModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n        \n    def forward(self, xb):\n        xb = xb.reshape(-1, 784)\n        out = self.linear(xb)\n        return out\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc.detach()}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n    \nmodel = MnistModel()\n\n\n\n##Training\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\ndef evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:51.431026Z","iopub.execute_input":"2024-07-20T15:50:51.431382Z","iopub.status.idle":"2024-07-20T15:50:51.445979Z","shell.execute_reply.started":"2024-07-20T15:50:51.431353Z","shell.execute_reply":"2024-07-20T15:50:51.444700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(model, val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:51.447547Z","iopub.execute_input":"2024-07-20T15:50:51.448300Z","iopub.status.idle":"2024-07-20T15:50:52.533692Z","shell.execute_reply.started":"2024-07-20T15:50:51.448266Z","shell.execute_reply":"2024-07-20T15:50:52.532564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = fit(5, 0.001, model, train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:50:52.535017Z","iopub.execute_input":"2024-07-20T15:50:52.535348Z","iopub.status.idle":"2024-07-20T15:51:26.084072Z","shell.execute_reply.started":"2024-07-20T15:50:52.535321Z","shell.execute_reply":"2024-07-20T15:51:26.082826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = [r['val_acc'] for r in history]\nplt.plot(accuracies, '-x')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy vs. No. of epochs');","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.085514Z","iopub.execute_input":"2024-07-20T15:51:26.085954Z","iopub.status.idle":"2024-07-20T15:51:26.413063Z","shell.execute_reply.started":"2024-07-20T15:51:26.085915Z","shell.execute_reply":"2024-07-20T15:51:26.411880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction\ndef predict_image(img, model):\n    xb = img.unsqueeze(0)\n    yb = model(xb)\n    _, preds  = torch.max(yb, dim=1)\n    return preds[0].item()\nimg, label = test_ds[919]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.414572Z","iopub.execute_input":"2024-07-20T15:51:26.415085Z","iopub.status.idle":"2024-07-20T15:51:26.662985Z","shell.execute_reply.started":"2024-07-20T15:51:26.415049Z","shell.execute_reply":"2024-07-20T15:51:26.661852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tensorflow","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.version)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.664854Z","iopub.execute_input":"2024-07-20T15:51:26.665278Z","iopub.status.idle":"2024-07-20T15:51:26.670702Z","shell.execute_reply.started":"2024-07-20T15:51:26.665243Z","shell.execute_reply":"2024-07-20T15:51:26.669676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scalar=tf.constant([[69,5],[6,7],[33,4]])\nscalar.ndim\nprint(scalar)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.671972Z","iopub.execute_input":"2024-07-20T15:51:26.672659Z","iopub.status.idle":"2024-07-20T15:51:26.682095Z","shell.execute_reply.started":"2024-07-20T15:51:26.672624Z","shell.execute_reply":"2024-07-20T15:51:26.680905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scalar","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.683914Z","iopub.execute_input":"2024-07-20T15:51:26.684374Z","iopub.status.idle":"2024-07-20T15:51:26.694240Z","shell.execute_reply.started":"2024-07-20T15:51:26.684338Z","shell.execute_reply":"2024-07-20T15:51:26.693199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scalarr=tf.variable_creator_scope(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.695697Z","iopub.execute_input":"2024-07-20T15:51:26.696067Z","iopub.status.idle":"2024-07-20T15:51:26.704396Z","shell.execute_reply.started":"2024-07-20T15:51:26.696036Z","shell.execute_reply":"2024-07-20T15:51:26.703250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#creating random tensor\nrandom =tf.random.Generator.from_seed(33)\n#set seed for reproducibiltiy\nrandom.normal(shape=(33,2))\n#normal outputs random values from a normal distribution\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.705884Z","iopub.execute_input":"2024-07-20T15:51:26.706687Z","iopub.status.idle":"2024-07-20T15:51:26.750725Z","shell.execute_reply.started":"2024-07-20T15:51:26.706647Z","shell.execute_reply":"2024-07-20T15:51:26.749671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"attributes of tensors\n\ndatatype of every element\n\nnumber of dimensions\n\nshape of tensor\n\nelements along the 0 axis\n\nelements along the last axis\n\ntotal number of elements in our tensor\n","metadata":{}},{"cell_type":"markdown","source":"Indexing tensors","metadata":{}},{"cell_type":"code","source":"somelist=[1,23,45,6]\nsomelist[0:]","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.752161Z","iopub.execute_input":"2024-07-20T15:51:26.752565Z","iopub.status.idle":"2024-07-20T15:51:26.760322Z","shell.execute_reply.started":"2024-07-20T15:51:26.752531Z","shell.execute_reply":"2024-07-20T15:51:26.759188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting the first two elements of each ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.761826Z","iopub.execute_input":"2024-07-20T15:51:26.762227Z","iopub.status.idle":"2024-07-20T15:51:26.767842Z","shell.execute_reply.started":"2024-07-20T15:51:26.762192Z","shell.execute_reply":"2024-07-20T15:51:26.766652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nm=tf.constant([[45.,4],[5,6]])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.769078Z","iopub.execute_input":"2024-07-20T15:51:26.769466Z","iopub.status.idle":"2024-07-20T15:51:26.778179Z","shell.execute_reply.started":"2024-07-20T15:51:26.769431Z","shell.execute_reply":"2024-07-20T15:51:26.777127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.math.reduce_variance(m)\ntf.math.reduce_max(m)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.779639Z","iopub.execute_input":"2024-07-20T15:51:26.780059Z","iopub.status.idle":"2024-07-20T15:51:26.801130Z","shell.execute_reply.started":"2024-07-20T15:51:26.780023Z","shell.execute_reply":"2024-07-20T15:51:26.800046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.argmin(m)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.802594Z","iopub.execute_input":"2024-07-20T15:51:26.802972Z","iopub.status.idle":"2024-07-20T15:51:26.815358Z","shell.execute_reply.started":"2024-07-20T15:51:26.802940Z","shell.execute_reply":"2024-07-20T15:51:26.814398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.one_hot(somelist,depth=2)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.816561Z","iopub.execute_input":"2024-07-20T15:51:26.816898Z","iopub.status.idle":"2024-07-20T15:51:26.828970Z","shell.execute_reply.started":"2024-07-20T15:51:26.816870Z","shell.execute_reply":"2024-07-20T15:51:26.827679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Architecture of a neural network regression model\n* input layer shape-same shape as the no. of features\n* hidden layer-problem specific\n* neurons per hidden layer-problem specific\n* output layer shape- same shape as desired predicition shape\n* hidden activation-usually reLu(rectified linear unit)\n* output activation-\n* loss function-MAE,MSE\n* optimizer-SGD,Adam","metadata":{}},{"cell_type":"code","source":"tf.constant(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.830331Z","iopub.execute_input":"2024-07-20T15:51:26.830818Z","iopub.status.idle":"2024-07-20T15:51:26.837755Z","shell.execute_reply.started":"2024-07-20T15:51:26.830790Z","shell.execute_reply":"2024-07-20T15:51:26.836701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# regression with neural networks in tensorflow","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Create features (using tensors)\nX = tf.constant([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0],dtype=tf.float32)\n\n# Create labels (using tensors)\ny = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0],dtype=tf.float32)\n\n# Visualize it\nplt.scatter(X, y);","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:26.839206Z","iopub.execute_input":"2024-07-20T15:51:26.839689Z","iopub.status.idle":"2024-07-20T15:51:27.079924Z","shell.execute_reply.started":"2024-07-20T15:51:26.839654Z","shell.execute_reply":"2024-07-20T15:51:27.078832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set random seed\ntf.random.set_seed(42)\n\n#create a model using the sequential API\nmodel= tf.keras.Sequential([tf.keras.layers.Dense(1)])\n#compile the model\nmodel.compile(loss=tf.keras.losses.mae,optimizer=tf.keras.optimizers.SGD(),metrics=['mae'])\n#fit the model\nmodel.fit(tf.expand_dims(X, axis=-1), y, epochs=1000,verbose=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:27.081369Z","iopub.execute_input":"2024-07-20T15:51:27.081784Z","iopub.status.idle":"2024-07-20T15:51:51.531951Z","shell.execute_reply.started":"2024-07-20T15:51:27.081752Z","shell.execute_reply":"2024-07-20T15:51:51.530856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X,y","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:51.533294Z","iopub.execute_input":"2024-07-20T15:51:51.533683Z","iopub.status.idle":"2024-07-20T15:51:51.541854Z","shell.execute_reply.started":"2024-07-20T15:51:51.533650Z","shell.execute_reply":"2024-07-20T15:51:51.540591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1=tf.constant([12,4,0,2,9])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:51.543299Z","iopub.execute_input":"2024-07-20T15:51:51.543673Z","iopub.status.idle":"2024-07-20T15:51:51.552236Z","shell.execute_reply.started":"2024-07-20T15:51:51.543637Z","shell.execute_reply":"2024-07-20T15:51:51.550970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict([y1])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:51.553921Z","iopub.execute_input":"2024-07-20T15:51:51.554429Z","iopub.status.idle":"2024-07-20T15:51:51.671714Z","shell.execute_reply.started":"2024-07-20T15:51:51.554370Z","shell.execute_reply":"2024-07-20T15:51:51.670691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"improving the model\n* increase the number of hidden units(all called neurons) within each of the hidden layers,change the activation function of each layer\n* changing the optimization function or perhaphs the learning rate of the optimization function.\n* fit the model for more epochs or on more data","metadata":{}},{"cell_type":"code","source":"X1=np.arange(-1000,1000,4)\ny1=np.arange(-990,1010,4)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:51.673080Z","iopub.execute_input":"2024-07-20T15:51:51.673425Z","iopub.status.idle":"2024-07-20T15:51:51.679336Z","shell.execute_reply.started":"2024-07-20T15:51:51.673396Z","shell.execute_reply":"2024-07-20T15:51:51.677939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yy=X1+10==y1","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:51.681535Z","iopub.execute_input":"2024-07-20T15:51:51.682356Z","iopub.status.idle":"2024-07-20T15:51:51.694429Z","shell.execute_reply.started":"2024-07-20T15:51:51.682311Z","shell.execute_reply":"2024-07-20T15:51:51.693072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"splitting data in training set, validation set and  test set\nhere we only split in train and test set","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain=X1[:400]\nytrain=y1[:400]\n\nxtest=X1[400:]\nytest=y1[400:]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:51.695730Z","iopub.execute_input":"2024-07-20T15:51:51.696186Z","iopub.status.idle":"2024-07-20T15:51:51.708684Z","shell.execute_reply.started":"2024-07-20T15:51:51.696147Z","shell.execute_reply":"2024-07-20T15:51:51.707416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))\n# Plot training data in blue\nplt.scatter(xtrain, ytrain, c='b', label='Training data')\n# Plot test data in green\nplt.scatter(xtest, ytest, c='g', label='Testing data')\n# Show the legend\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:51.710244Z","iopub.execute_input":"2024-07-20T15:51:51.710630Z","iopub.status.idle":"2024-07-20T15:51:52.099221Z","shell.execute_reply.started":"2024-07-20T15:51:51.710578Z","shell.execute_reply":"2024-07-20T15:51:52.097936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random seed\ntf.random.set_seed(42)\n\n# Replicate model_1 and add an extra layer\nmodel_2 = tf.keras.Sequential([\n  tf.keras.layers.Dense(50),\n  tf.keras.layers.Dense(1) # add a second layer\n])\n\n# Compile the model\nmodel_2.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=['mae'])\n\n# Fit the model\nmodel_2.fit(tf.expand_dims(xtrain, axis=-1), ytrain, epochs=100, verbose=0) # set verbose to 0 for less output\n     \n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:52.100672Z","iopub.execute_input":"2024-07-20T15:51:52.101121Z","iopub.status.idle":"2024-07-20T15:51:57.056581Z","shell.execute_reply.started":"2024-07-20T15:51:52.101056Z","shell.execute_reply":"2024-07-20T15:51:57.055405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds=model.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:57.057901Z","iopub.execute_input":"2024-07-20T15:51:57.058235Z","iopub.status.idle":"2024-07-20T15:51:57.202465Z","shell.execute_reply.started":"2024-07-20T15:51:57.058204Z","shell.execute_reply":"2024-07-20T15:51:57.201307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_predictions(train_data=xtrain, \n                     train_labels=ytrain, \n                     test_data=xtest, \n                     test_labels=ytest, \n                     predictions=y_preds):\n  \"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", label=\"Training data\")\n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", label=\"Testing data\")\n  # Plot the predictions in red (predictions were made on the test data)\n  plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")\n  # Show the legend\n  plt.legend();\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:57.204168Z","iopub.execute_input":"2024-07-20T15:51:57.204604Z","iopub.status.idle":"2024-07-20T15:51:57.212048Z","shell.execute_reply.started":"2024-07-20T15:51:57.204574Z","shell.execute_reply":"2024-07-20T15:51:57.210364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" plot_predictions(train_data=xtrain, \n                     train_labels=ytrain, \n                     test_data=xtest, \n                     test_labels=ytest, \n                     predictions=y_preds)\n  ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:57.213606Z","iopub.execute_input":"2024-07-20T15:51:57.214112Z","iopub.status.idle":"2024-07-20T15:51:57.631091Z","shell.execute_reply.started":"2024-07-20T15:51:57.214068Z","shell.execute_reply":"2024-07-20T15:51:57.629718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluating model'\nthe cycle of building a model, fitting of model,evaluate the model, tweak the model and again fit it","metadata":{}},{"cell_type":"code","source":"model.evaluate(xtest,ytest)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:57.632681Z","iopub.execute_input":"2024-07-20T15:51:57.633068Z","iopub.status.idle":"2024-07-20T15:51:57.833952Z","shell.execute_reply.started":"2024-07-20T15:51:57.633037Z","shell.execute_reply":"2024-07-20T15:51:57.832684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:57.835659Z","iopub.execute_input":"2024-07-20T15:51:57.836089Z","iopub.status.idle":"2024-07-20T15:51:57.862704Z","shell.execute_reply.started":"2024-07-20T15:51:57.836052Z","shell.execute_reply":"2024-07-20T15:51:57.861596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification with neural networks \nTypical architecture of a classification neural network\nThe word typical is on purpose.\n\nBecause the architecture of a classification neural network can widely vary depending on the problem you're working on.\n\nHowever, there are some fundamentals all deep neural networks contain:\n\nAn input layer.\nSome hidden layers.\nAn output layer.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_circles\n#make 1000eg\nn_samples=1000\n#create circles\nx,y=make_circles(n_samples,noise=0.03,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:57.864069Z","iopub.execute_input":"2024-07-20T15:51:57.864420Z","iopub.status.idle":"2024-07-20T15:51:58.371345Z","shell.execute_reply.started":"2024-07-20T15:51:57.864391Z","shell.execute_reply":"2024-07-20T15:51:58.370174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.DataFrame({'X0':x[:,0],'X1':x[:,1],'label':y})\ndf","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:58.372704Z","iopub.execute_input":"2024-07-20T15:51:58.373026Z","iopub.status.idle":"2024-07-20T15:51:58.402441Z","shell.execute_reply.started":"2024-07-20T15:51:58.373000Z","shell.execute_reply":"2024-07-20T15:51:58.401271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.RdYlBu)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:58.403816Z","iopub.execute_input":"2024-07-20T15:51:58.404156Z","iopub.status.idle":"2024-07-20T15:51:58.678209Z","shell.execute_reply.started":"2024-07-20T15:51:58.404128Z","shell.execute_reply":"2024-07-20T15:51:58.677104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Steps in modelling**\n\nNow we know what data we have as well as the input and output shapes, let's see how we'd build a neural network to model it.\n\nIn TensorFlow, there are typically 3 fundamental steps to creating and training a model.\n\nCreating a model - piece together the layers of a neural network yourself (using the functional or sequential API) or import a previously built model (known as transfer learning).\n\nCompiling a model - defining how a model's performance should be measured (loss/metrics) as well as defining how it should improve (optimizer).\n\nFitting a model - letting the model try to find patterns in the data (how does X get to y).","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntf.random.set_seed(42)\n\nmodel_1= tf.keras.Sequential([\n  tf.keras.layers.Dense(100,activation='relu')\n    ,tf.keras.layers.Dense(1)\n])\n\n# 2. Compile the model\nmodel_1.compile(loss=tf.keras.losses.BinaryCrossentropy(), # binary since we are working with 2 clases (0 & 1)\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['accuracy'])\n\n# 3. Fit the model\nmodel_1.fit(x, y, epochs=300,verbose=0)\n      ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:51:58.679712Z","iopub.execute_input":"2024-07-20T15:51:58.680085Z","iopub.status.idle":"2024-07-20T15:52:17.961130Z","shell.execute_reply.started":"2024-07-20T15:51:58.680053Z","shell.execute_reply":"2024-07-20T15:52:17.959967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.evaluate(x,y)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:17.962643Z","iopub.execute_input":"2024-07-20T15:52:17.962995Z","iopub.status.idle":"2024-07-20T15:52:18.202249Z","shell.execute_reply.started":"2024-07-20T15:52:17.962964Z","shell.execute_reply":"2024-07-20T15:52:18.200989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds=model_1.predict(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:18.203929Z","iopub.execute_input":"2024-07-20T15:52:18.204391Z","iopub.status.idle":"2024-07-20T15:52:18.400491Z","shell.execute_reply.started":"2024-07-20T15:52:18.204351Z","shell.execute_reply":"2024-07-20T15:52:18.399256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random seed\ntf.random.set_seed(42)\n\n# Create a model\nmodel_2 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, ReLU activation\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, ReLU activation\n  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) # ouput layer, sigmoid activation\n])\n\n# Compile the model\nmodel_2.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=['accuracy'])\n\n# Fit the model\nhistory = model_2.fit(x, y, epochs=100, verbose=0)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:18.402372Z","iopub.execute_input":"2024-07-20T15:52:18.402779Z","iopub.status.idle":"2024-07-20T15:52:26.832728Z","shell.execute_reply.started":"2024-07-20T15:52:18.402747Z","shell.execute_reply":"2024-07-20T15:52:26.831776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y3=model_2.predict(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:26.834205Z","iopub.execute_input":"2024-07-20T15:52:26.834654Z","iopub.status.idle":"2024-07-20T15:52:27.046666Z","shell.execute_reply.started":"2024-07-20T15:52:26.834596Z","shell.execute_reply":"2024-07-20T15:52:27.045443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef plot_decision_boundary(model, X, y):\n  # Define the axis boundaries of the plot and create a meshgrid\n  x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                       np.linspace(y_min, y_max, 100))\n\n  # Create X values (we're going to predict on all of these)\n  x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together: https://numpy.org/devdocs/reference/generated/numpy.c_.html\n\n  # Make predictions using the trained model\n  y_pred = model.predict(x_in)\n\n  # Check for multi-class\n  if model.output_shape[-1] > 1: # checks the final dimension of the model's output shape, if this is > (greater than) 1, it's multi-class\n    print(\"doing multiclass classification...\")\n    # We have to reshape our predictions to get them ready for plotting\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n  else:\n    print(\"doing binary classifcation...\")\n    y_pred = np.round(np.max(y_pred, axis=1)).reshape(xx.shape)\n\n  # Plot decision boundary\n  plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n  plt.xlim(xx.min(), xx.max())\n  plt.ylim(yy.min(), yy.max())\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:27.048428Z","iopub.execute_input":"2024-07-20T15:52:27.048791Z","iopub.status.idle":"2024-07-20T15:52:27.059693Z","shell.execute_reply.started":"2024-07-20T15:52:27.048760Z","shell.execute_reply":"2024-07-20T15:52:27.058475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_decision_boundary(model_2,x,y)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:27.061243Z","iopub.execute_input":"2024-07-20T15:52:27.061847Z","iopub.status.idle":"2024-07-20T15:52:27.911741Z","shell.execute_reply.started":"2024-07-20T15:52:27.061810Z","shell.execute_reply":"2024-07-20T15:52:27.910573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Activation functions","metadata":{}},{"cell_type":"code","source":"# Create a toy tensor (similar to the data we pass into our model)\nA = tf.cast(tf.range(-10, 10), tf.float32)\nA\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:27.913316Z","iopub.execute_input":"2024-07-20T15:52:27.913752Z","iopub.status.idle":"2024-07-20T15:52:27.926395Z","shell.execute_reply.started":"2024-07-20T15:52:27.913712Z","shell.execute_reply":"2024-07-20T15:52:27.925136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize our toy tensor\nplt.plot(A);","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:27.928092Z","iopub.execute_input":"2024-07-20T15:52:27.928923Z","iopub.status.idle":"2024-07-20T15:52:28.155735Z","shell.execute_reply.started":"2024-07-20T15:52:27.928835Z","shell.execute_reply":"2024-07-20T15:52:28.154570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now let's recreate the sigmoid function and see what it does to our data\nwe can also use the pre defined sigmoid function at tf.keras.actiavtions.sigmoid\n\nSigmoid takes a real value as input and outputs another value between 0 and 1. Its easy to work with and has all the nice properties of activation functions: its non-linear, continuously differentiable, monotonic, and has a fixed output range.\n\nPros\n\nIt is nonlinear in nature. Combinations of this function are also nonlinear!\nIt will give an analog activation unlike step function.\nIt has a smooth gradient too.\nIts good for a classifier.\nThe output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. So we have our activations bound in a range. Nice, it wont blow up the activations then.\nCons\n\nTowards either end of the sigmoid function, the Y values tend to respond very less to changes in X.\nIt gives rise to a problem of vanishing gradients.\nIts output isnt zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\nSigmoids saturate and kill gradients.\nThe network refuses to learn further or is drastically slow ( depending on use case and until gradient /computation gets hit by floating point value limits ).","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1/(1+tf.exp(-x))\n\nsigmoid(A)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:28.166392Z","iopub.execute_input":"2024-07-20T15:52:28.166793Z","iopub.status.idle":"2024-07-20T15:52:28.179526Z","shell.execute_reply.started":"2024-07-20T15:52:28.166761Z","shell.execute_reply":"2024-07-20T15:52:28.178422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets plot it","metadata":{}},{"cell_type":"code","source":"plt.plot(sigmoid(A))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:28.180878Z","iopub.execute_input":"2024-07-20T15:52:28.181213Z","iopub.status.idle":"2024-07-20T15:52:28.441269Z","shell.execute_reply.started":"2024-07-20T15:52:28.181187Z","shell.execute_reply":"2024-07-20T15:52:28.439796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"a non linear line!\n\nlets check relu function(turns all negatives to zero and positive numbers stay the same)\n\nA recent invention which stands for Rectified Linear Units. The formula is deceptively simple: max(0,z)\n. Despite its name and appearance, its not linear and provides the same benefits as Sigmoid (i.e. the ability to learn nonlinear functions), but with better performance.","metadata":{}},{"cell_type":"code","source":"def relu(x):\n    return tf.maximum(0,x)\n\nrelu(A)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:28.442820Z","iopub.execute_input":"2024-07-20T15:52:28.443196Z","iopub.status.idle":"2024-07-20T15:52:28.454312Z","shell.execute_reply.started":"2024-07-20T15:52:28.443164Z","shell.execute_reply":"2024-07-20T15:52:28.453133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(relu(A))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:28.456515Z","iopub.execute_input":"2024-07-20T15:52:28.456973Z","iopub.status.idle":"2024-07-20T15:52:28.746025Z","shell.execute_reply.started":"2024-07-20T15:52:28.456939Z","shell.execute_reply":"2024-07-20T15:52:28.744646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"what about linear activation function","metadata":{}},{"cell_type":"code","source":"tf.keras.activations.linear(A)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:28.747599Z","iopub.execute_input":"2024-07-20T15:52:28.748091Z","iopub.status.idle":"2024-07-20T15:52:28.756429Z","shell.execute_reply.started":"2024-07-20T15:52:28.748049Z","shell.execute_reply":"2024-07-20T15:52:28.755285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"A==tf.keras.activations.linear(A)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:28.757784Z","iopub.execute_input":"2024-07-20T15:52:28.758775Z","iopub.status.idle":"2024-07-20T15:52:28.770398Z","shell.execute_reply.started":"2024-07-20T15:52:28.758736Z","shell.execute_reply":"2024-07-20T15:52:28.769160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tanh squashes a real-valued number to the range [-1, 1]. Its non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.","metadata":{}},{"cell_type":"code","source":"def tanh(x):\n    return(tf.exp(x)-tf.exp(-x)/((tf.exp(x)+tf.exp(-x))))\n\ntanh(A)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:28.772220Z","iopub.execute_input":"2024-07-20T15:52:28.772635Z","iopub.status.idle":"2024-07-20T15:52:28.787829Z","shell.execute_reply.started":"2024-07-20T15:52:28.772587Z","shell.execute_reply":"2024-07-20T15:52:28.786357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(tanh(A))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:28.789389Z","iopub.execute_input":"2024-07-20T15:52:28.789848Z","iopub.status.idle":"2024-07-20T15:52:29.084431Z","shell.execute_reply.started":"2024-07-20T15:52:28.789795Z","shell.execute_reply":"2024-07-20T15:52:29.082068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, so it makes sense now the model doesn't really learn anything when using only linear activation functions, because the linear activation function doesn't change our input data in anyway.\n\nWhere as, with our non-linear functions, our data gets manipulated. A neural network uses these kind of transformations at a large scale to figure draw patterns between its inputs and outputs.","metadata":{}},{"cell_type":"markdown","source":"# Multiclass classification with neural network","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\n\n# The data has already been sorted into training and test sets for us\n(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:29.086243Z","iopub.execute_input":"2024-07-20T15:52:29.086783Z","iopub.status.idle":"2024-07-20T15:52:30.303973Z","shell.execute_reply.started":"2024-07-20T15:52:29.086737Z","shell.execute_reply":"2024-07-20T15:52:30.302720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[1]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:30.305201Z","iopub.execute_input":"2024-07-20T15:52:30.305521Z","iopub.status.idle":"2024-07-20T15:52:30.315901Z","shell.execute_reply.started":"2024-07-20T15:52:30.305495Z","shell.execute_reply":"2024-07-20T15:52:30.314644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, 60,000 training examples each with shape (28, 28) and a label each as well as 10,000 test examples of shape (28, 28).\n\nBut these are just numbers, let's visualize.","metadata":{}},{"cell_type":"code","source":"plt.imshow(train_data[22])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:30.317568Z","iopub.execute_input":"2024-07-20T15:52:30.318010Z","iopub.status.idle":"2024-07-20T15:52:30.528753Z","shell.execute_reply.started":"2024-07-20T15:52:30.317970Z","shell.execute_reply":"2024-07-20T15:52:30.527561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# How many classes are there (this'll be our output shape)?\nlen(class_names)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:30.530442Z","iopub.execute_input":"2024-07-20T15:52:30.530934Z","iopub.status.idle":"2024-07-20T15:52:30.540371Z","shell.execute_reply.started":"2024-07-20T15:52:30.530881Z","shell.execute_reply":"2024-07-20T15:52:30.538977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot multiple random images of fashion MNIST\nimport random\nplt.figure(figsize=(7, 7))\nfor i in range(4):\n  ax = plt.subplot(2, 2, i + 1)\n  rand_index = random.choice(range(len(train_data)))\n  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n  plt.title(class_names[train_labels[rand_index]])\n  plt.axis(False)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:30.541902Z","iopub.execute_input":"2024-07-20T15:52:30.542307Z","iopub.status.idle":"2024-07-20T15:52:31.348286Z","shell.execute_reply.started":"2024-07-20T15:52:30.542278Z","shell.execute_reply":"2024-07-20T15:52:31.346495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The input shape will have to deal with 28x28 tensors (the height and width of our images).\nWe're actually going to squash the input into a tensor (vector) of shape (784).\nThe output shape will have to be 10 because we need our model to predict for 10 different classes.","metadata":{}},{"cell_type":"markdown","source":"We'll also change the activation parameter of our output layer to be \"softmax\" instead of 'sigmoid'. As we'll see the \"softmax\" activation function outputs a series of values between 0 & 1 (the same shape as output shape, which together add up to ~1. The index with the highest value is predicted by the model to be the most likely class.\n\n\n\nWe'll need to change our loss function from a binary loss function to a multiclass loss function.\nMore specifically, since our labels are in integer form, \n\nwe'll use tf.keras.losses.SparseCategoricalCrossentropy(), if our labels were one-hot encoded (e.g. they looked something like [0, 0, 1, 0, 0...]), we'd use tf.keras.losses.CategoricalCrossentropy().\n\n\nWe'll also use the validation_data parameter when calling the fit() function. This will give us an idea of how the model performs on the test set during training.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n#set random seed\ntf.random.set_seed(42)\n#build the model\nmodel_m=tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28,28)),\n    tf.keras.layers.Dense(4,activation='relu'),\n    tf.keras.layers.Dense(4,activation='relu'),\n    tf.keras.layers.Dense(10,activation='softmax')\n])\n#had to reshape 28x28 to 784, and output shape is 10, activation is softmax\n#compile the model\nmodel_m.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n               optimizer=tf.keras.optimizers.Adam(),\n               metrics=['accuracy'])\nnon_norm_history=model_m.fit(train_data,train_labels,epochs=50,validation_data=(test_data,test_labels),verbose=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:52:31.349929Z","iopub.execute_input":"2024-07-20T15:52:31.350353Z","iopub.status.idle":"2024-07-20T15:54:59.842933Z","shell.execute_reply.started":"2024-07-20T15:52:31.350318Z","shell.execute_reply":"2024-07-20T15:54:59.841710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the shapes of our model \nmodel_m.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:54:59.845089Z","iopub.execute_input":"2024-07-20T15:54:59.845473Z","iopub.status.idle":"2024-07-20T15:54:59.870223Z","shell.execute_reply.started":"2024-07-20T15:54:59.845438Z","shell.execute_reply":"2024-07-20T15:54:59.869172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.min()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:54:59.871519Z","iopub.execute_input":"2024-07-20T15:54:59.871877Z","iopub.status.idle":"2024-07-20T15:54:59.882986Z","shell.execute_reply.started":"2024-07-20T15:54:59.871849Z","shell.execute_reply":"2024-07-20T15:54:59.881861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"right now, the data we have isn't between 0 and 1, in other words, it's not normalized (hence why we used the non_norm_history variable when calling fit()). It's pixel values are between 0 and 255.","metadata":{}},{"cell_type":"markdown","source":"We can get these values between 0 and 1 by dividing the entire array by the maximum: 255.0 (dividing by a float also converts to a float).\n\nDoing so will result in all of our data being between 0 and 1 (known as scaling or normalization).","metadata":{}},{"cell_type":"code","source":"train_data=train_data/train_data.max()\ntest_data=test_data/test_data.max()\ntrain_data.max(),test_data.max()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:54:59.884283Z","iopub.execute_input":"2024-07-20T15:54:59.884628Z","iopub.status.idle":"2024-07-20T15:55:00.130948Z","shell.execute_reply.started":"2024-07-20T15:54:59.884582Z","shell.execute_reply":"2024-07-20T15:55:00.129656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"yummers!!! now our data is between 0 and 1. let's use the above model with the normalized data","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n#set random seed\ntf.random.set_seed(42)\n#build the model\nmodel_m2=tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28,28)),\n    tf.keras.layers.Dense(4,activation='relu'),\n    tf.keras.layers.Dense(4,activation='relu'),\n    tf.keras.layers.Dense(10,activation='softmax')\n])\n#had to reshape 28x28 to 784, and output shape is 10, activation is softmax\n#compile the model\nmodel_m2.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n               optimizer=tf.keras.optimizers.Adam(),\n               metrics=['accuracy'])\nnorm_history=model_m2.fit(train_data,train_labels,epochs=50,validation_data=(test_data,test_labels),verbose=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T16:26:13.210233Z","iopub.execute_input":"2024-07-20T16:26:13.211208Z","iopub.status.idle":"2024-07-20T16:28:29.597559Z","shell.execute_reply.started":"2024-07-20T16:26:13.211171Z","shell.execute_reply":"2024-07-20T16:28:29.596401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_m2.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T16:30:55.274501Z","iopub.execute_input":"2024-07-20T16:30:55.274926Z","iopub.status.idle":"2024-07-20T16:30:55.945300Z","shell.execute_reply.started":"2024-07-20T16:30:55.274890Z","shell.execute_reply":"2024-07-20T16:30:55.944198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(non_norm_history.history).plot(title='Non-normalized Data')\npd.DataFrame(norm_history.history).plot(title='Normalized data')","metadata":{"execution":{"iopub.status.busy":"2024-07-20T16:31:05.724839Z","iopub.execute_input":"2024-07-20T16:31:05.725248Z","iopub.status.idle":"2024-07-20T16:31:06.367513Z","shell.execute_reply.started":"2024-07-20T16:31:05.725216Z","shell.execute_reply":"2024-07-20T16:31:06.366363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's find the ideal  learning rate ","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nmodel_m3=tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28,28)),\n    tf.keras.layers.Dense(4,activation='relu'),\n    tf.keras.layers.Dense(4,activation='relu'),\n    tf.keras.layers.Dense(10,activation='softmax')\n])\nmodel_m3.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=[\"accuracy\"])\nlr_scheduler=tf.keras.callbacks.LearningRateScheduler(lambda epoch:1e-3*10**(epoch/20))\nfind_lr_history = model_m3.fit(train_data,\n                               train_labels,\n                               epochs=40, # model already doing pretty good with current LR, probably don't need 100 epochs\n                               validation_data=(test_data, test_labels),\n                               callbacks=[lr_scheduler],verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T17:00:05.254866Z","iopub.execute_input":"2024-07-20T17:00:05.255626Z","iopub.status.idle":"2024-07-20T17:02:18.163470Z","shell.execute_reply.started":"2024-07-20T17:00:05.255565Z","shell.execute_reply":"2024-07-20T17:02:18.162328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the learning rate decay curve\nimport numpy as np\nimport matplotlib.pyplot as plt\nlrs = 1e-3 * (10**(np.arange(40)/20))\nplt.semilogx(lrs, find_lr_history.history[\"loss\"]) # want the x-axis to be log-scale\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Loss\")\nplt.title(\"Finding the ideal learning rate\");\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T17:29:47.983889Z","iopub.execute_input":"2024-07-20T17:29:47.984641Z","iopub.status.idle":"2024-07-20T17:29:48.480985Z","shell.execute_reply.started":"2024-07-20T17:29:47.984589Z","shell.execute_reply":"2024-07-20T17:29:48.479691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we've got a model trained with a close-to-ideal learning rate and performing pretty well, we've got a couple of options.\n\nWe could:\n\nEvaluate its performance using other classification metrics (such as a confusion matrix or classification report).\nAssess some of its predictions (through visualizations).\nImprove its accuracy (by training it for longer or changing the architecture).\nSave and export it for use in an application.\nLet's go through the first two options.\n\nFirst we'll create a classification matrix to visualize its predictions across the different classes.","metadata":{}},{"cell_type":"code","source":"# Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_m4 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_m4.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # ideal learning rate (same as default)\n                 metrics=[\"accuracy\"])\n\n# Fit the model\nhistory = model_m4.fit(train_data,\n                       train_labels,\n                       epochs=100,\n                       validation_data=(test_data, test_labels),verbose=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:18:54.898629Z","iopub.execute_input":"2024-07-20T18:18:54.899339Z","iopub.status.idle":"2024-07-20T18:23:30.179294Z","shell.execute_reply.started":"2024-07-20T18:18:54.899300Z","shell.execute_reply":"2024-07-20T18:23:30.178095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_probs=model_m4.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:33:44.162540Z","iopub.execute_input":"2024-07-20T18:33:44.163312Z","iopub.status.idle":"2024-07-20T18:33:44.678193Z","shell.execute_reply.started":"2024-07-20T18:33:44.163276Z","shell.execute_reply":"2024-07-20T18:33:44.676923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the predicted class number and label for the first example\ny_probs[0].argmax(), class_names[y_probs[0].argmax()]","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:33:45.215375Z","iopub.execute_input":"2024-07-20T18:33:45.216411Z","iopub.status.idle":"2024-07-20T18:33:45.223266Z","shell.execute_reply.started":"2024-07-20T18:33:45.216371Z","shell.execute_reply":"2024-07-20T18:33:45.222205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds=y_probs.argmax(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:33:47.335962Z","iopub.execute_input":"2024-07-20T18:33:47.336344Z","iopub.status.idle":"2024-07-20T18:33:47.341875Z","shell.execute_reply.started":"2024-07-20T18:33:47.336315Z","shell.execute_reply":"2024-07-20T18:33:47.340685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds[:100]","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:33:48.714711Z","iopub.execute_input":"2024-07-20T18:33:48.715101Z","iopub.status.idle":"2024-07-20T18:33:48.723175Z","shell.execute_reply.started":"2024-07-20T18:33:48.715055Z","shell.execute_reply":"2024-07-20T18:33:48.722009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check out the non-prettified confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_true=test_labels,\n                 y_pred=y_preds)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:33:49.643704Z","iopub.execute_input":"2024-07-20T18:33:49.644083Z","iopub.status.idle":"2024-07-20T18:33:49.655708Z","shell.execute_reply.started":"2024-07-20T18:33:49.644055Z","shell.execute_reply":"2024-07-20T18:33:49.654572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\n# Create a function for plotting a random image along with its prediction\ndef plot_random_image(model, images, true_labels, classes):\n  \"\"\"Picks a random image, plots it and labels it with a predicted and truth label.\n\n  Args:\n    model: a trained model (trained on data similar to what's in images).\n    images: a set of random images (in tensor form).\n    true_labels: array of ground truth labels for images.\n    classes: array of class names for images.\n\n  Returns:\n    A plot of a random image from `images` with a predicted class label from `model`\n    as well as the truth class label from `true_labels`.\n  \"\"\"\n  # Setup random integer\n  i = random.randint(0, len(images))\n\n  # Create predictions and targets\n  target_image = images[i]\n  pred_probs = model.predict(target_image.reshape(1, 28, 28)) # have to reshape to get into right size for model\n  pred_label = classes[pred_probs.argmax()]\n  true_label = classes[true_labels[i]]\n\n  # Plot the target image\n  plt.imshow(target_image, cmap=plt.cm.binary)\n\n  # Change the color of the titles depending on if the prediction is right or wrong\n  if pred_label == true_label:\n    color = \"green\"\n  else:\n    color = \"red\"\n\n  # Add xlabel information (prediction/true label)\n  plt.xlabel(\"Pred: {} {:2.0f}% (True: {})\".format(pred_label,\n                                                   100*tf.reduce_max(pred_probs),\n                                                   true_label),\n             color=color) # set the color to green or red\n     \n\n    \n  \nimport random\n\n# Create a function for plotting a random image along with its prediction\ndef plot_random_image(model, images, true_labels, classes):\n  \"\"\"Picks a random image, plots it and labels it with a predicted and truth label.\n\n  Args:\n    model: a trained model (trained on data similar to what's in images).\n    images: a set of random images (in tensor form).\n    true_labels: array of ground truth labels for images.\n    classes: array of class names for images.\n\n  Returns:\n    A plot of a random image from `images` with a predicted class label from `model`\n    as well as the truth class label from `true_labels`.\n  \"\"\"\n  # Setup random integer\n  i = random.randint(0, len(images))\n\n  # Create predictions and targets\n  target_image = images[i]\n  pred_probs = model.predict(target_image.reshape(1, 28, 28)) # have to reshape to get into right size for model\n  pred_label = classes[pred_probs.argmax()]\n  true_label = classes[true_labels[i]]\n\n  # Plot the target image\n  plt.imshow(target_image, cmap=plt.cm.binary)\n\n  # Change the color of the titles depending on if the prediction is right or wrong\n  if pred_label == true_label:\n    color = \"green\"\n  else:\n    color = \"red\"\n\n  # Add xlabel information (prediction/true label)\n  plt.xlabel(\"Pred: {} {:2.0f}% (True: {})\".format(pred_label,\n                                                   100*tf.reduce_max(pred_probs),\n                                                   true_label),\n             color=color) # set the color to green or red\n     \n\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:33:50.796948Z","iopub.execute_input":"2024-07-20T18:33:50.797329Z","iopub.status.idle":"2024-07-20T18:33:50.808316Z","shell.execute_reply.started":"2024-07-20T18:33:50.797300Z","shell.execute_reply":"2024-07-20T18:33:50.807170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:33:52.164932Z","iopub.execute_input":"2024-07-20T18:33:52.165306Z","iopub.status.idle":"2024-07-20T18:33:52.170344Z","shell.execute_reply.started":"2024-07-20T18:33:52.165278Z","shell.execute_reply":"2024-07-20T18:33:52.169259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_random_image(model_m4,images=test_data,true_labels=test_labels,classes=class_names)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:35:45.435364Z","iopub.execute_input":"2024-07-20T18:35:45.435770Z","iopub.status.idle":"2024-07-20T18:35:45.742720Z","shell.execute_reply.started":"2024-07-20T18:35:45.435737Z","shell.execute_reply":"2024-07-20T18:35:45.741674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A model learns by updating and improving its weight matrices and biases values every epoch (in our case, when we call the fit() fucntion).\n\nIt does so by comparing the patterns its learned between the data and labels to the actual labels.\n\nIf the current patterns (weight matrices and bias values) don't result in a desirable decrease in the loss function (higher loss means worse predictions), the optimizer tries to steer the model to update its patterns in the right way (using the real labels as a reference).\n\nThis process of using the real labels as a reference to improve the model's predictions is called backpropagation.\n\nIn other words, data and labels pass through a model (forward pass) and it attempts to learn the relationship between the data and labels.\n\nAnd if this learned relationship isn't close to the actual relationship or it could be improved, the model does so by going back through itself (backward pass) and tweaking its weights matrices and bias values to better represent the data.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}