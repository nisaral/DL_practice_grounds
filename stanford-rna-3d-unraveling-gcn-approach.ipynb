{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87793,"databundleVersionId":11228175,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß¨ Stanford RNA 3D Folding Competition: Structure Prediction Masterclass üß†\n\n<a href=\"https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding\" target=\"_blank\">\n  <img src=\"https://img.shields.io/badge/Kaggle-Competition-blue?style=for-the-badge&logo=kaggle\" alt=\"Kaggle Competition\">\n</a>\n\nüåü **Welcome to the Ultimate RNA 3D Structure Prediction Notebook!** üåü\n\n*Unlock the secrets of RNA folding through graph deep learning. This comprehensive guide combines biological insights with advanced AI techniques to tackle one of molecular biology's most challenging problems.*\n\n---\n\n## üöÄ Notebook Roadmap: From Sequence to Structure\n\n<div style=\"padding: 15px; border: 2px solid #2ecc71; border-radius: 10px; margin: 20px 0;\">\nüîç **Section 1: Exploratory Data Analysis (EDA)**  \n   - üìå 3D coordinate distribution analysis  \n   - üìå Sequence pattern visualization  \n   - üìå Structural thermodynamics insights\n</div>\n\n<div style=\"padding: 15px; border: 2px solid #3498db; border-radius: 10px; margin: 20px 0;\">\nüîß **Section 2: Data Preprocessing Pipeline**  \n   - üß¨ RNA sequence encoding  \n   - üéØ Coordinate normalization & outlier detection  \n   - üß© Dynamic padding for variable-length sequences\n</div>\n\n<div style=\"padding: 15px; border: 2px solid #9b59b6; border-radius: 10px; margin: 20px 0;\">\nü§ñ <strong>Section 3: Graph Neural Network Architecture</strong>  \n</div>\n\n<div style=\"padding: 15px; border: 2px solid #e67e22; border-radius: 10px; margin: 20px 0;\">\n‚öôÔ∏è <strong>Section 4: Model Training Strategies</strong><br>  \nüéØ Loss: Weighted MAE + Structural Consistency<br>  \n‚è±Ô∏è Early stopping with 3D validation<br>  \nüîÑ Gradient clipping (norm=1.0)  \n</div>\n","metadata":{"_uuid":"ba75c96f-68e8-4eb7-8159-5d762f35366f","_cell_guid":"4cd7fda1-faf2-4318-8c84-a231a515b338","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 1. Data  Loading and Exploration","metadata":{"_uuid":"4974e6d3-499d-4e64-ab6f-eb6b1fd4ba99","_cell_guid":"2e35b03c-2c0b-49fe-945d-e632cd4e7eeb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# TensorFlow/Keras for deep learning model\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, Conv1D, BatchNormalization, Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\n#For GCN MODEL\nfrom spektral.layers import GCNConv\nfrom spektral.utils import adjacency_to_edge_list\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"_uuid":"1c8a6414-5cce-4cf9-82dc-74c2b1937448","_cell_guid":"95be3c1b-d91d-4ff2-aeb1-16b50fa1936e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-04T09:12:07.455872Z","iopub.execute_input":"2025-03-04T09:12:07.456184Z","iopub.status.idle":"2025-03-04T09:12:19.999043Z","shell.execute_reply.started":"2025-03-04T09:12:07.456159Z","shell.execute_reply":"2025-03-04T09:12:19.998379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nWe load the CSV files provided in the competition:\n- `train_sequences.csv`\n- `train_labels.csv`\n- `validation_sequences.csv` & `validation_labels.csv`\n- `test_sequences.csv`\n- `sample_submission.csv`\n\n","metadata":{"_uuid":"7d1c7eed-82d4-46ea-859c-d7a7490d9852","_cell_guid":"08036a25-432d-4eca-bbfe-961e87c3f83a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"TRAIN_SEQ_PATH = '/kaggle/input/stanford-rna-3d-folding/train_sequences.csv'\nTRAIN_LABELS_PATH = '/kaggle/input/stanford-rna-3d-folding/train_labels.csv'\nVALID_SEQ_PATH = '/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv'\nVALID_LABELS_PATH = '/kaggle/input/stanford-rna-3d-folding/validation_labels.csv'\nTEST_SEQ_PATH  = '/kaggle/input/stanford-rna-3d-folding/test_sequences.csv'\nSAMPLE_SUB_PATH = '/kaggle/input/stanford-rna-3d-folding/sample_submission.csv'\n\ntrain_sequences = pd.read_csv(TRAIN_SEQ_PATH)\ntrain_labels = pd.read_csv(TRAIN_LABELS_PATH)\nvalid_sequences = pd.read_csv(VALID_SEQ_PATH)\nvalid_labels = pd.read_csv(VALID_LABELS_PATH)\ntest_sequences = pd.read_csv(TEST_SEQ_PATH)\nsample_submission = pd.read_csv(SAMPLE_SUB_PATH)\n\ntrain_labels.fillna(0, inplace=True)\nvalid_labels.fillna(0, inplace=True)\n\nprint(\"Train Sequences Shape:\", train_sequences.shape)\nprint(\"Train Labels Shape:\", train_labels.shape)\nprint(\"Validation Sequences Shape:\", valid_sequences.shape)\nprint(\"Validation Labels Shape:\", valid_labels.shape)\nprint(\"Test Sequences Shape:\", test_sequences.shape)\n\nprint(\"\\nTrain Sequences Head:\")\nprint(train_sequences.head())\nprint(\"\\nTrain Labels Head:\")\nprint(train_labels.head())","metadata":{"_uuid":"bd1b8c77-f907-4a3b-af02-5141a285d2ac","_cell_guid":"9f130e5f-7f0d-424f-8634-f304c96c9437","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-04T09:12:25.126174Z","iopub.execute_input":"2025-03-04T09:12:25.126971Z","iopub.status.idle":"2025-03-04T09:12:25.574472Z","shell.execute_reply.started":"2025-03-04T09:12:25.126931Z","shell.execute_reply":"2025-03-04T09:12:25.573512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**histogram of sequence lengths across train, validation, and test sets**\nThe model pads sequences to a maximum length (4298 in the training set), but understanding the distribution can inform padding strategies or model architecture (e.g., handling variable lengths better).","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.hist(train_sequences['sequence'].str.len(), bins=50, alpha=0.7, label='Train')\nplt.hist(valid_sequences['sequence'].str.len(), bins=50, alpha=0.7, label='Validation')\nplt.hist(test_sequences['sequence'].str.len(), bins=50, alpha=0.7, label='Test')\nplt.xlabel('Sequence Length')\nplt.ylabel('Frequency')\nplt.title('Distribution of RNA Sequence Lengths')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:12:40.607722Z","iopub.execute_input":"2025-03-04T09:12:40.608014Z","iopub.status.idle":"2025-03-04T09:12:41.097326Z","shell.execute_reply.started":"2025-03-04T09:12:40.607993Z","shell.execute_reply":"2025-03-04T09:12:41.096549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Calculating and visualizing the proportion of each nucleotide**\n\nThe frequency of A, C, G, and U might influence folding patterns or model bias.\n","metadata":{}},{"cell_type":"code","source":"from collections import Counter\ntrain_nucleotides = ''.join(train_sequences['sequence'])\ncounts = Counter(train_nucleotides)\nplt.bar(counts.keys(), counts.values())\nplt.title('Nucleotide Composition in Training Sequences')\nplt.xlabel('Nucleotide')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:12:57.209667Z","iopub.execute_input":"2025-03-04T09:12:57.209985Z","iopub.status.idle":"2025-03-04T09:12:57.386042Z","shell.execute_reply.started":"2025-03-04T09:12:57.209959Z","shell.execute_reply":"2025-03-04T09:12:57.385209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**histograms or boxplots of coordinates and compute statistics**\n\nUnderstanding the range and spread of x, y, z coordinates can help assess data scale and whether normalization is needed.","metadata":{}},{"cell_type":"code","source":"coords = np.vstack([train_labels_dict[tid] for tid in train_labels_dict])\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, coord in enumerate(['x_1', 'y_1', 'z_1']):\n    axes[i].hist(coords[:, i], bins=50)\n    axes[i].set_title(f'Distribution of {coord}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:13:10.602114Z","iopub.execute_input":"2025-03-04T09:13:10.602414Z","iopub.status.idle":"2025-03-04T09:13:11.452995Z","shell.execute_reply.started":"2025-03-04T09:13:10.602391Z","shell.execute_reply":"2025-03-04T09:13:11.451870Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Convert to datetime and explore trends over time**","metadata":{}},{"cell_type":"code","source":"train_sequences['temporal_cutoff'] = pd.to_datetime(train_sequences['temporal_cutoff'])\nplt.figure(figsize=(10, 6))\ntrain_sequences['temporal_cutoff'].dt.year.value_counts().sort_index().plot(kind='bar')\nplt.title('Sequences by Year of Temporal Cutoff')\nplt.xlabel('Year')\nplt.ylabel('Number of Sequences')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:13:24.026147Z","iopub.execute_input":"2025-03-04T09:13:24.026421Z","iopub.status.idle":"2025-03-04T09:13:24.385512Z","shell.execute_reply.started":"2025-03-04T09:13:24.026399Z","shell.execute_reply":"2025-03-04T09:13:24.384701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Preprocessing\n\n### Sequence Encoding\n\nWe map each nucleotide to an integer:\n- A: 1, C: 2, G: 3, U: 4  \nUnknown characters are mapped to 0.","metadata":{"_uuid":"7b3ae69d-4427-4c9f-931e-9ea1176f5886","_cell_guid":"f84ddf2c-b665-420f-a7ec-43cfe711c0a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"nucleotide_map = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n\ndef encode_sequence(seq):\n    \"\"\"Encodes a RNA sequence into a list of integers based on nucleotide_map.\"\"\"\n    return [nucleotide_map.get(ch, 0) for ch in seq]\n\n# Apply encoding to all sequence files\ntrain_sequences['encoded'] = train_sequences['sequence'].apply(encode_sequence)\nvalid_sequences['encoded'] = valid_sequences['sequence'].apply(encode_sequence)\ntest_sequences['encoded'] = test_sequences['sequence'].apply(encode_sequence)","metadata":{"_uuid":"2f38e291-5974-4c43-93a0-fb024c343ba0","_cell_guid":"3c95facf-6580-4e0d-bf92-9cfb9539a551","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-04T09:12:29.940995Z","iopub.execute_input":"2025-03-04T09:12:29.941316Z","iopub.status.idle":"2025-03-04T09:12:29.960926Z","shell.execute_reply.started":"2025-03-04T09:12:29.941289Z","shell.execute_reply":"2025-03-04T09:12:29.960173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"** Processing Label Data**\n\nEach row in the labels CSV is for one residue, with an `ID` formatted as `target_id_resid`.\nWe group rows by `target_id` and sort by residue number.\nHere, we use the first structure (x_1, y_1, z_1) as our target coordinates.","metadata":{"_uuid":"fbc2c63d-cb94-493c-9d4f-e0cf078d07c1","_cell_guid":"c3189e08-66bf-4548-8f33-3422b765f88c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def process_labels(labels_df):\n    \"\"\"\n    Processes a labels DataFrame by grouping rows by target_id.\n    Returns a dictionary mapping target_id to an array of coordinates (seq_len, 3).\n    \"\"\"\n    label_dict = {}\n    for idx, row in labels_df.iterrows():\n        # Split ID into target_id and residue number (assumes format \"targetid_resid\")\n        parts = row['ID'].split('_')\n        target_id = \"_\".join(parts[:-1])\n        resid = int(parts[-1])\n        # Extract the coordinates; they should be numeric (missing values already set to 0)\n        coord = np.array([row['x_1'], row['y_1'], row['z_1']], dtype=np.float32)\n        if target_id not in label_dict:\n            label_dict[target_id] = []\n        label_dict[target_id].append((resid, coord))\n    \n    # Sort residues by resid and stack coordinates\n    for key in label_dict:\n        sorted_coords = sorted(label_dict[key], key=lambda x: x[0])\n        coords = np.stack([c for r, c in sorted_coords])\n        label_dict[key] = coords\n    return label_dict\n\n# Process training and validation labels\ntrain_labels_dict = process_labels(train_labels)\nvalid_labels_dict = process_labels(valid_labels)","metadata":{"_uuid":"1d94dbc2-d120-4c82-9d70-711e6b4a68c1","_cell_guid":"afbd9f74-c685-4e0a-9262-005656f79ca9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-04T09:12:32.395788Z","iopub.execute_input":"2025-03-04T09:12:32.396080Z","iopub.status.idle":"2025-03-04T09:12:39.628621Z","shell.execute_reply.started":"2025-03-04T09:12:32.396057Z","shell.execute_reply":"2025-03-04T09:12:39.627986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nucleotide_map = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n\ndef encode_sequence(seq):\n    return [nucleotide_map.get(ch, 0) for ch in seq]\n\ntrain_sequences['encoded'] = train_sequences['sequence'].apply(encode_sequence)\nvalid_sequences['encoded'] = valid_sequences['sequence'].apply(encode_sequence)\ntest_sequences['encoded'] = test_sequences['sequence'].apply(encode_sequence)\n\ndef process_labels(labels_df):\n    label_dict = {}\n    for idx, row in labels_df.iterrows():\n        parts = row['ID'].split('_')\n        target_id = \"_\".join(parts[:-1])\n        resid = int(parts[-1])\n        coord = np.array([row['x_1'], row['y_1'], row['z_1']], dtype=np.float32)\n        if target_id not in label_dict:\n            label_dict[target_id] = []\n        label_dict[target_id].append((resid, coord))\n    for key in label_dict:\n        sorted_coords = sorted(label_dict[key], key=lambda x: x[0])\n        coords = np.stack([c for r, c in sorted_coords])\n        label_dict[key] = coords\n    return label_dict\n\ntrain_labels_dict = process_labels(train_labels)\nvalid_labels_dict = process_labels(valid_labels)\n\ndef create_graph_dataset(sequences_df, labels_dict):\n    X_list, A_list, y_list, target_ids = [], [], [], []\n    for idx, row in sequences_df.iterrows():\n        tid = row['target_id']\n        if tid in labels_dict:\n            seq = row['encoded']\n            seq_len = len(seq)\n            # Node features (n_nodes, n_features)\n            X = np.array(seq, dtype=np.float32).reshape(-1, 1)  # Shape: [seq_len, 1]\n            # Adjacency matrix for backbone (n_nodes, n_nodes)\n            A = np.zeros((seq_len, seq_len), dtype=np.float32)\n            for i in range(seq_len - 1):\n                A[i, i + 1] = 1\n                A[i + 1, i] = 1  # Undirected graph\n            # Labels\n            y = labels_dict[tid]  # Shape: [seq_len, 3]\n            X_list.append(X)\n            A_list.append(A)\n            y_list.append(y)\n            target_ids.append(tid)\n    return X_list, A_list, y_list, target_ids\n\nX_train, A_train, y_train, train_ids = create_graph_dataset(train_sequences, train_labels_dict)\nX_valid, A_valid, y_valid, valid_ids = create_graph_dataset(valid_sequences, valid_labels_dict)\n\nmax_len = max(len(seq) for seq in X_train)\nprint(\"Maximum sequence length (train):\", max_len)\n\ndef pad_graph(X, A, y, max_len):\n    seq_len = X.shape[0]\n    if seq_len < max_len:\n        # Pad node features\n        X_pad = np.pad(X, ((0, max_len - seq_len), (0, 0)), mode='constant', constant_values=0)\n        # Pad adjacency matrix\n        A_pad = np.pad(A, ((0, max_len - seq_len), (0, max_len - seq_len)), mode='constant', constant_values=0)\n        # Pad coordinates\n        y_pad = np.pad(y, ((0, max_len - seq_len), (0, 0)), mode='constant', constant_values=0)\n    else:\n        X_pad, A_pad, y_pad = X, A, y\n    return X_pad, A_pad, y_pad\n\nX_train_pad = []\nA_train_pad = []\ny_train_pad = []\nfor x, a, y in zip(X_train, A_train, y_train):\n    x_p, a_p, y_p = pad_graph(x, a, y, max_len)\n    X_train_pad.append(x_p)\n    A_train_pad.append(a_p)\n    y_train_pad.append(y_p)\n\nX_valid_pad = []\nA_valid_pad = []\ny_valid_pad = []\nfor x, a, y in zip(X_valid, A_valid, y_valid):\n    x_p, a_p, y_p = pad_graph(x, a, y, max_len)\n    X_valid_pad.append(x_p)\n    A_valid_pad.append(a_p)\n    y_valid_pad.append(y_p)\n\nX_train_pad = np.array(X_train_pad)  # Shape: [n_samples, max_len, 1]\nA_train_pad = np.array(A_train_pad)  # Shape: [n_samples, max_len, max_len]\ny_train_pad = np.array(y_train_pad)  # Shape: [n_samples, max_len, 3]\nX_valid_pad = np.array(X_valid_pad)\nA_valid_pad = np.array(A_valid_pad)\ny_valid_pad = np.array(y_valid_pad)\n\nprint(\"X_train_pad shape:\", X_train_pad.shape)\nprint(\"A_train_pad shape:\", A_train_pad.shape)\nprint(\"y_train_pad shape:\", y_train_pad.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T10:01:45.156343Z","iopub.execute_input":"2025-03-04T10:01:45.156631Z","execution_failed":"2025-03-04T10:02:07.778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Creating Datasets and Padding**\n\nWe match each target sequence with its corresponding coordinate labels.\nThen we pad sequences and coordinate arrays to a uniform length.\n\nPadded positions in coordinates are set to 0.","metadata":{"_uuid":"acfc3ec8-97ac-44cf-a2dc-d1a721b1c8d8","_cell_guid":"1508c42f-9b88-4934-be7d-c75fcc36195c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def create_dataset(sequences_df, labels_dict):\n    \"\"\"\n    Creates a dataset from a sequences DataFrame and a labels dictionary.\n    Returns:\n        X: list of encoded sequences,\n        y: list of coordinate arrays,\n        target_ids: list of target ids.\n    \"\"\"\n    X, y, target_ids = [], [], []\n    for idx, row in sequences_df.iterrows():\n        tid = row['target_id']\n        if tid in labels_dict:\n            X.append(row['encoded'])\n            y.append(labels_dict[tid])\n            target_ids.append(tid)\n    return X, y, target_ids\n\n# Create training and validation datasets\nX_train, y_train, train_ids = create_dataset(train_sequences, train_labels_dict)\nX_valid, y_valid, valid_ids = create_dataset(valid_sequences, valid_labels_dict)\n\n# Determine maximum sequence length from training set\nmax_len = max(len(seq) for seq in X_train)\nprint(\"Maximum sequence length (train):\", max_len)\n\n# Pad the sequences (padding value = 0)\nX_train_pad = pad_sequences(X_train, maxlen=max_len, padding='post', value=0)\nX_valid_pad = pad_sequences(X_valid, maxlen=max_len, padding='post', value=0)\n\n# Function to pad coordinate arrays\ndef pad_coordinates(coord_array, max_len):\n    L = coord_array.shape[0]\n    if L < max_len:\n        pad_width = ((0, max_len - L), (0, 0))\n        return np.pad(coord_array, pad_width, mode='constant', constant_values=0)\n    else:\n        return coord_array\n\n# Pad coordinate arrays\ny_train_pad = np.array([pad_coordinates(arr, max_len) for arr in y_train])\ny_valid_pad = np.array([pad_coordinates(arr, max_len) for arr in y_valid])\n\n# Check for any NaN values in the targets\nprint(\"Any NaN in y_train_pad?\", np.isnan(y_train_pad).any())\nprint(\"Any NaN in y_valid_pad?\", np.isnan(y_valid_pad).any())\n\nprint(\"X_train_pad shape:\", X_train_pad.shape)\nprint(\"y_train_pad shape:\", y_train_pad.shape)","metadata":{"_uuid":"f00ec075-2695-461e-a423-b8b251c43acd","_cell_guid":"be76be8c-682f-418d-94d0-4f6bb519b344","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-04T09:12:39.629869Z","iopub.execute_input":"2025-03-04T09:12:39.630174Z","iopub.status.idle":"2025-03-04T09:12:39.788790Z","shell.execute_reply.started":"2025-03-04T09:12:39.630147Z","shell.execute_reply":"2025-03-04T09:12:39.788059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.1 Basic CNN Model Training\n\nIn this section, we build a basic CNN-based model.\nThe model uses:\n- An Embedding layer  \n- Two Conv1D blocks (with BatchNormalization and Dropout)  \n- A final Conv1D layer (kernel size 1) to output 3 coordinates per residue","metadata":{"_uuid":"8d832ed7-a808-47b1-9bd1-4ca8aa72b337","_cell_guid":"9616835f-b774-4de4-bb7d-99fffe07dc65","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Define hyperparameters for the CNN model\nvocab_size = max(nucleotide_map.values()) + 1  # +1 for padding token 0\nembedding_dim = 16\nnum_filters = 64\nkernel_size = 3\ndrop_rate = 0.2\n\n# Build the CNN model\ninput_seq_cnn = Input(shape=(max_len,), name='input_seq')\nx_cnn = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True, name='embedding')(input_seq_cnn)\n\n# First convolutional block\nx_cnn = Conv1D(filters=num_filters, kernel_size=kernel_size, padding='same', activation='relu', name='conv1')(x_cnn)\nx_cnn = BatchNormalization(name='bn1')(x_cnn)\nx_cnn = Dropout(drop_rate, name='drop1')(x_cnn)\n\n# Second convolutional block\nx_cnn = Conv1D(filters=num_filters, kernel_size=kernel_size, padding='same', activation='relu', name='conv2')(x_cnn)\nx_cnn = BatchNormalization(name='bn2')(x_cnn)\nx_cnn = Dropout(drop_rate, name='drop2')(x_cnn)\n\n# Final convolution to output 3 coordinates per residue (x, y, z)\noutput_coords_cnn = Conv1D(filters=3, kernel_size=1, padding='same', activation='linear', name='predicted_coords')(x_cnn)\n\ncnn_model = Model(inputs=input_seq_cnn, outputs=output_coords_cnn)\ncnn_model.compile(optimizer='adam', loss='mse')\n\ncnn_model.summary()","metadata":{"_uuid":"b5b77c10-dad8-4527-8f28-174c04afefe2","_cell_guid":"eb0e2b5f-347a-45b9-a475-a2a4108dd9f1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-03T10:40:29.749471Z","iopub.execute_input":"2025-03-03T10:40:29.749857Z","iopub.status.idle":"2025-03-03T10:40:31.997396Z","shell.execute_reply.started":"2025-03-03T10:40:29.749823Z","shell.execute_reply":"2025-03-03T10:40:31.996579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Model Training\n\nWe train the CNN model using early stopping to monitor the validation loss.\nWith the NaN issues addressed in the data, training should proceed without nan losses.","metadata":{"_uuid":"93c354e3-05d7-4632-89cb-b9c843c37977","_cell_guid":"7744df17-746a-438d-82eb-3fda8874d03c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"early_stop_cnn = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\nhistory_cnn = cnn_model.fit(X_train_pad, y_train_pad,\n                            validation_data=(X_valid_pad, y_valid_pad),\n                            epochs=50,\n                            batch_size=16,\n                            callbacks=[early_stop_cnn],\n                            verbose=1)\n\n# Plot training and validation loss\nplt.figure(figsize=(8, 5))\nplt.plot(history_cnn.history['loss'], label='Train Loss (CNN)')\nplt.plot(history_cnn.history['val_loss'], label='Val Loss (CNN)')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE Loss\")\nplt.title(\"CNN Model Training vs. Validation Loss\")\nplt.legend()\nplt.show()","metadata":{"_uuid":"8b6e7d19-9670-4f06-9995-61f156f5588e","_cell_guid":"fbd23461-6579-49ed-adba-4c414b97a74b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-03T10:40:31.998273Z","iopub.execute_input":"2025-03-03T10:40:31.998617Z","iopub.status.idle":"2025-03-03T10:40:43.972050Z","shell.execute_reply.started":"2025-03-03T10:40:31.998584Z","shell.execute_reply":"2025-03-03T10:40:43.971357Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. GCN model building and training","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nvocab_size = max(nucleotide_map.values()) + 1  # 5 (0 for padding)\nembedding_dim = 16\ngcn_units = 64\ndrop_rate = 0.2\n\n# Build GCN model\ninput_nodes = Input(shape=(max_len, 1), name='input_nodes')  # Node features\ninput_adj = Input(shape=(max_len, max_len), name='input_adj')  # Adjacency matrix\n\n# Embedding layer\nx = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(input_nodes)  # [n_samples, max_len, embedding_dim]\nx = tf.squeeze(x, axis=2)  # Remove feature dim if 1, now [n_samples, max_len, embedding_dim]\n\n# GCN layers\nx = GCNConv(gcn_units, activation='relu')([x, input_adj])\nx = Dropout(drop_rate)(x)\nx = GCNConv(gcn_units, activation='relu')([x, input_adj])\nx = Dropout(drop_rate)(x)\n\n# Output layer: predict 3 coordinates per node\noutput_coords = Dense(3, activation='linear', name='predicted_coords')(x)  # [n_samples, max_len, 3]\n\ngcn_model = Model(inputs=[input_nodes, input_adj], outputs=output_coords)\n\n# Custom masked loss to ignore padded regions\ndef masked_mse(y_true, y_pred):\n    mask = tf.cast(tf.reduce_any(tf.not_equal(y_true, 0), axis=-1), tf.float32)\n    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n    masked_mse = tf.reduce_sum(mse * mask) / tf.reduce_sum(mask)\n    return masked_mse\n\ngcn_model.compile(optimizer='adam', loss=masked_mse)\ngcn_model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\nhistory = gcn_model.fit(\n    [X_train_pad, A_train_pad], y_train_pad,\n    validation_data=([X_valid_pad, A_valid_pad], y_valid_pad),\n    epochs=100,\n    batch_size=32,\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Plot training history\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Generating Predictions and Submission File\n\nFor each test sequence, we predict the 3D coordinates using our trained CNN model.\n\nThe submission requires 5 sets of coordinates per target. In this baseline, we replicate the same predicted structure 5 times.","metadata":{"_uuid":"fe88f530-9cd2-4dd5-94f2-41e2021c50f9","_cell_guid":"fe7b78f7-a78f-4cda-b597-5cbf25d98f28","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Prepare test data: pad sequences to same length as training\nX_test = test_sequences['encoded'].tolist()\nX_test_pad = pad_sequences(X_test, maxlen=max_len, padding='post', value=0)\n\n# Predict coordinates using the trained CNN model\npredictions = cnn_model.predict(X_test_pad)\n\n# Build submission rows. Each row corresponds to a residue from a test target.\nsubmission_rows = []\nfor idx, row in test_sequences.iterrows():\n    target_id = row['target_id']\n    # Get predicted coordinates (shape: [max_len, 3])\n    pred_coords = predictions[idx]\n    # Determine actual sequence length\n    seq_length = len(row['encoded'])\n    pred_coords = pred_coords[:seq_length, :]  # only actual residues\n    \n    # For each residue, create a row in the submission file\n    for i in range(seq_length):\n        coords = pred_coords[i, :]\n        # Replicate the same prediction 5 times for submission format\n        submission_rows.append({\n            'ID': f\"{target_id}_{i+1}\",\n            'resname': row['sequence'][i],\n            'resid': i+1,\n            **{f\"x_{j+1}\": coords[0] for j in range(5)},\n            **{f\"y_{j+1}\": coords[1] for j in range(5)},\n            **{f\"z_{j+1}\": coords[2] for j in range(5)}\n        })\n\nsubmission_df = pd.DataFrame(submission_rows)\nprint(\"Submission DataFrame shape:\", submission_df.shape)\nprint(submission_df.head(10))","metadata":{"_uuid":"d1e938fc-98b7-499c-b05b-b6edd621c9d0","_cell_guid":"fbab4072-6a39-49e3-b6f7-57e64556e9d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-03T10:40:43.973705Z","iopub.execute_input":"2025-03-03T10:40:43.973922Z","iopub.status.idle":"2025-03-03T10:40:44.320886Z","shell.execute_reply.started":"2025-03-03T10:40:43.973904Z","shell.execute_reply":"2025-03-03T10:40:44.320116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"generating predictions for GCN model ","metadata":{}},{"cell_type":"code","source":"# Prepare test data\nX_test = test_sequences['encoded'].tolist()\nA_test = [np.zeros((len(seq), len(seq))) for seq in X_test]\nfor i, seq in enumerate(X_test):\n    for j in range(len(seq) - 1):\n        A_test[i][j, j + 1] = 1\n        A_test[i][j + 1, j] = 1\n\nX_test_pad = []\nA_test_pad = []\nfor x, a in zip(X_test, A_test):\n    x_p = np.pad(x, (0, max_len - len(x)), mode='constant', constant_values=0).reshape(-1, 1)\n    a_p = np.pad(a, ((0, max_len - len(a)), (0, max_len - len(a))), mode='constant', constant_values=0)\n    X_test_pad.append(x_p)\n    A_test_pad.append(a_p)\n\nX_test_pad = np.array(X_test_pad)\nA_test_pad = np.array(A_test_pad)\n\n# Predict coordinates\npredictions = gcn_model.predict([X_test_pad, A_test_pad])\n\n# Build submission\nsubmission_rows = []\nfor idx, row in test_sequences.iterrows():\n    target_id = row['target_id']\n    pred_coords = predictions[idx]\n    seq_length = len(row['encoded'])\n    pred_coords = pred_coords[:seq_length, :]\n    \n    for i in range(seq_length):\n        coords = pred_coords[i, :]\n        submission_rows.append({\n            'ID': f\"{target_id}_{i+1}\",\n            'resname': row['sequence'][i],\n            'resid': i+1,\n            **{f\"x_{j+1}\": coords[0] for j in range(5)},\n            **{f\"y_{j+1}\": coords[1] for j in range(5)},\n            **{f\"z_{j+1}\": coords[2] for j in range(5)}\n        })\n\nsubmission_df = pd.DataFrame(submission_rows)\nprint(\"Submission DataFrame shape:\", submission_df.shape)\nprint(submission_df.head(10))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Saving the Submission File\n\nFinally, we save the submission file as `submission.csv`.","metadata":{"_uuid":"7d9bbfd9-5f22-46bb-9a74-86c64c32623e","_cell_guid":"e122805e-51a2-411f-81f8-072ff85e4f5a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file saved as submission.csv\")","metadata":{"_uuid":"2632f190-329c-4373-9632-b03b99d89e90","_cell_guid":"f1b1e66f-6ed2-4fed-b7bc-b5b9d2a306c2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-03T10:40:44.321886Z","iopub.execute_input":"2025-03-03T10:40:44.322195Z","iopub.status.idle":"2025-03-03T10:40:44.365493Z","shell.execute_reply.started":"2025-03-03T10:40:44.322166Z","shell.execute_reply":"2025-03-03T10:40:44.364790Z"}},"outputs":[],"execution_count":null}]}